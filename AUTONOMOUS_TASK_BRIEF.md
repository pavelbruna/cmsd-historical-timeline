# ğŸ¤– AUTONOMOUS TASK: CMSD Historical Data Extraction

**For: Claude Code Terminal**  
**Mode: Full Autonomy** ğŸš€

---

## ğŸ“‹ YOUR MISSION

Extract structured historical data from Czech PDF infographics into a PostgreSQL database.

**You have complete freedom on:**
- Architecture & approach
- Tools & libraries
- Database design
- Extraction methodology
- Quality assurance

**Just deliver the results!**

---

## ğŸ“ INPUT FILES

```
/project/pdfs/
â”œâ”€â”€ 1L.pdf, 1R.pdf          # Creation, flood (biblical)
â”œâ”€â”€ 2L.pdf, 2R.pdf          # Biblical chronology
â”œâ”€â”€ 3L.pdf, 3R.pdf          # Ancient history
â”œâ”€â”€ 4L.pdf, 4R.pdf          # Classical antiquity
â”œâ”€â”€ 5L.pdf, 5R.pdf          # Middle Ages
â”œâ”€â”€ 6L.pdf, 6R.pdf          # Early modern
â”œâ”€â”€ 7L.pdf, 7R.pdf          # Modern era
â”œâ”€â”€ 8L.pdf, 8R.pdf          # Contemporary
â”œâ”€â”€ PredniPreds.pdf         # Introduction
â”œâ”€â”€ zadniPredsLic.pdf       # Ten Commandments
â””â”€â”€ zadniPredsRub.pdf       # Daniel prophecy

/project/knowledge_cards/
â”œâ”€â”€ cmsd_cards_batch1.jsonl
â”œâ”€â”€ cmsd_cards_batch2.jsonl
â”œâ”€â”€ cmsd_cards_batch3.jsonl
â”œâ”€â”€ cmsd_cards_batch4.jsonl
â””â”€â”€ cmsd_cards_batch5.jsonl
```

**19 PDFs + 5 JSONL files with metadata**

---

## ğŸ¯ DELIVERABLES

### 0. GitHub Repository â­ NEW!
Create a new GitHub repository for this project:

**Repository Name:** `cmsd-historical-timeline`

**Description:** 
```
ChronologickÃ¡ mapa svÄ›tovÃ½ch dÄ›jin (CMSD) - Digital Database

Structured historical timeline database extracted from Czech historical infographics. 
Contains 1000+ events from Creation to modern times, with people, places, and biblical references.

Mobile app coming soon (React Native)!
```

**Repository Structure:**
```
cmsd-historical-timeline/
â”œâ”€â”€ README.md                  # Project overview, features, usage
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ pdfs/             # Original 19 PDF infographics
â”‚   â”‚   â””â”€â”€ knowledge_cards/  # Metadata JSONL files
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ events.csv
â”‚   â”‚   â”œâ”€â”€ people.csv
â”‚   â”‚   â”œâ”€â”€ places.csv
â”‚   â”‚   â””â”€â”€ timeline.json
â”‚   â””â”€â”€ database/
â”‚       â”œâ”€â”€ schema.sql
â”‚       â””â”€â”€ cmsd.db           # SQLite export
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py            # Main extraction pipeline
â”‚   â”œâ”€â”€ ocr_utils.py
â”‚   â”œâ”€â”€ llm_extraction.py
â”‚   â””â”€â”€ database.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ EXTRACTION_REPORT.md  # Data quality & coverage
â”‚   â”œâ”€â”€ DATABASE_SCHEMA.md    # Schema documentation
â”‚   â””â”€â”€ API.md                # Future API docs
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE                    # MIT or appropriate license
```

**README.md should include:**
- Project description (Czech historical timeline database)
- Context: Part of CMSD mobile app project (React Native)
- Features: 1000+ events, searchable, AI-ready
- Data structure & schema
- Usage examples (SQL queries)
- Future: Mobile app, AI integration, MCP server
- Credits to original infographic author

**Commit everything with meaningful messages!**

### 1. Populated Database
PostgreSQL or SQLite with structured historical events:
- Events (year, title, description, category, region...)
- People (name, birth/death years, role...)
- Places (name, type, coordinates...)
- Relations (event-people, event-places...)
- Knowledge cards imported

### 2. Data Exports
- `events.csv` - All extracted events
- `people.csv` - All people
- `places.csv` - All places  
- `timeline.json` - Full timeline data

### 3. Quality Report
- `EXTRACTION_REPORT.md` with:
  - Events extracted per page
  - Coverage statistics
  - Issues encountered
  - Sample queries & results

### 4. Working Code
- Extraction scripts (Python/Node/whatever you choose)
- Database schema
- README with usage instructions

---

## âœ… SUCCESS CRITERIA

- **GitHub repository created** with proper structure
- **All source PDFs committed** to repo (data/raw/pdfs/)
- **Minimum 500 events** extracted (target: 1000-2000)
- **All 19 PDFs** processed
- **Czech encoding** preserved (UTF-8, diacritics intact)
- **BC/AD years** handled correctly (negative for BC)
- **Searchable database** (by year, person, place, tags)
- **Clean data** (<5% duplicates)
- **Documented** (clear README + extraction report)
- **Professional README** describing the CMSD mobile app project

---

## ğŸ’¡ SUGGESTED APPROACH (Optional!)

You don't have to follow this - choose your own path!

**Option A: OCR + LLM Extraction**
1. pdf2image + Tesseract OCR (Czech lang)
2. GPT-4o/Claude for structured extraction
3. PostgreSQL for storage

**Option B: Direct Text Extraction**
1. PyMuPDF for text (faster if PDFs have text layer)
2. LLM extraction
3. Database population

**Option C: Vision + LLM**
1. Convert PDF to images
2. GPT-4o Vision for direct extraction
3. Skip OCR entirely!

**You decide!** Pick what works best.

---

## ğŸ”§ TOOLS AT YOUR DISPOSAL

**PDF Processing:**
- pdf2image, PyMuPDF, pdfplumber
- Tesseract OCR (Czech support)
- Poppler utils

**LLM APIs:**
- OpenAI GPT-4o (vision + text)
- Anthropic Claude (Sonnet/Opus)
- Both available via API

**Database:**
- PostgreSQL (recommended)
- SQLite (simpler, portable)
- Whatever you prefer!

**Languages:**
- Python (recommended for data work)
- Node.js (if you prefer)
- Mix & match!

---

## ğŸ“Š DATA STRUCTURE HINTS

### Expected Event Format:
```json
{
  "year": -2348,              // Negative = BC
  "year_end": null,           // For periods
  "title": "Potopa svÄ›ta",
  "description": "BiblickÃ¡ potopa za Noeho...",
  "category": "religion",     // war, discovery, politics...
  "region": "BlÃ­zkÃ½ vÃ½chod",
  "importance": 5,            // 1-5
  "tags": ["bible", "potopa", "noe"],
  "people": ["Noe"],
  "places": ["Ararat"],
  "bible_refs": ["Genesis 6-9"],
  "source_page": "2L"
}
```

### Knowledge Card Format:
```json
{
  "id": "cmsd_2019_2l",
  "doc_id": "2L",
  "title": "Noe a potopa svÄ›ta",
  "summary": "BiblickÃ¡ chronologie...",
  "topics": ["bible", "potopa"],
  "entities": {
    "people": ["Noe"],
    "places": ["Ararat"],
    "events": ["Potopa svÄ›ta"]
  }
}
```

Use knowledge cards for **context** when extracting!

---

## âš ï¸ IMPORTANT NOTES

1. **Language:** PDFs are in Czech
   - OCR needs Czech language (`-l ces`)
   - LLM should understand Czech context
   - Preserve Czech characters (Ä, Å¡, Å¾, Å™, Å¯, Ä›, Ã½, Ã¡...)

2. **Year Format:**
   - BC (pÅ™.n.l.) = negative numbers: -2348
   - AD (n.l.) = positive numbers: 1492
   - Biblical chronology uses different systems - normalize!

3. **Knowledge Cards:**
   - Some have empty entities - that's OK!
   - Extract from OCR/vision instead
   - Use cards for validation & context

4. **PDF Quality:**
   - May vary between pages
   - Implement fallback strategies
   - Test on 1-2 pages first!

---

## ğŸš€ EXECUTION PLAN

**You're autonomous! But here's a suggested flow:**

### Phase 1: Reconnaissance (30 min)
- Examine input files
- Test OCR on sample PDF
- Choose extraction approach
- Design database schema

### Phase 2: Pipeline Development (2-3 hours)
- Build extraction pipeline
- Test on 2-3 PDFs
- Iterate & improve
- Scale to all 19 PDFs

### Phase 3: Database Population (30 min)
- Create schema
- Insert extracted data
- Build relations
- Add indexes

### Phase 4: Validation (30 min)
- Quality checks
- Generate statistics
- Export CSVs
- Write report

### Phase 5: Documentation (15 min)
- Write README
- Document approach
- Provide usage examples

**Total: ~4-5 hours of work**

---

## ğŸ¯ AUTONOMOUS WORKFLOW

**I trust you to:**
1. âœ… Examine the inputs
2. âœ… Choose the best approach
3. âœ… Install needed dependencies
4. âœ… Build the pipeline
5. âœ… Handle errors gracefully
6. âœ… Validate results
7. âœ… Document everything
8. âœ… Deliver clean outputs

**Ask me ONLY if:**
- â“ You need external resources (API keys, etc.)
- â“ You hit a blocker you can't solve
- â“ You need clarification on requirements

**Otherwise: GO FOR IT!** ğŸ’ª

---

## ğŸ“ˆ PROGRESS REPORTING

Feel free to report progress at key milestones:

```
âœ… Phase 1 complete: Tested OCR on 1L.pdf
   â†’ Extracted 45 events
   â†’ Quality: Good
   â†’ Approach: GPT-4o Vision

âœ… Phase 2 complete: All PDFs processed
   â†’ Total: 1,247 events extracted
   â†’ 213 people, 87 places
   â†’ Issues: 3 PDFs had low quality (handled)

âœ… Phase 3 complete: Database populated
   â†’ PostgreSQL schema created
   â†’ All data inserted
   â†’ Indexes added

âœ… Phase 4 complete: Validation done
   â†’ Quality: 97% (38 duplicates removed)
   â†’ Coverage: All pages processed
   â†’ CSVs exported

âœ… DONE! See EXTRACTION_REPORT.md
```

---

## ğŸ BONUS POINTS (Optional)

If you have time/interest:

- ğŸŒŸ Create MCP server for querying the database
- ğŸŒŸ Add full-text search indexes
- ğŸŒŸ Generate sample visualizations (timeline chart)
- ğŸŒŸ Create API endpoints for the data
- ğŸŒŸ Build simple web UI for browsing

**But main deliverables come first!**

---

## ğŸ READY?

You have:
- âœ… Clear objective
- âœ… Input files
- âœ… Success criteria  
- âœ… Full autonomy
- âœ… All the tools you need

**Now go build something awesome!** ğŸš€

---

## ğŸ“ CONTACT

If you need me:
- Ask questions anytime
- Share progress when you want
- Request clarifications if needed

Otherwise: **I trust you to deliver!** ğŸ’ª

**Good luck, Claude Code!** ğŸ¯
